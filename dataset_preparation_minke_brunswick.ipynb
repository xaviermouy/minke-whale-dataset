{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reformating of the Brunswick minke annotations\n",
    "\n",
    "\n",
    "## Purpose of this notebook\n",
    "This notebook describes the steps involved in gathering, cleaning up and reorganizing the minke whale pulse train manual annotations that were performed by an PA-group intern. Data were fully analysed from 17-Dec-2016 to 27-Dec-2016 (channel 6 of deployment USA-NEFSC-GA-201612). This fully annotated dataset can be used to assess the performance of the detector (i.e. quantify precision, recall, and number of false alarms per day)\n",
    "\n",
    "The specific objectives of this notebook are:\n",
    "\n",
    "- Convert the Raven annotation tables to have annotation times relative to the beginning of each audio file.\n",
    "- Add metadata to all annotations (i.e., coordinates, depths, location, dates, etc)\n",
    "- Make annotation labels consistent (i.e. 'MW' for minke)\n",
    "\n",
    "\n",
    "## Adding annotation fields in Raven\n",
    "\n",
    "Since the times of the original annotations are relative to the first file of each day, we need to add additional fields so we can recalculate annotation times relative to the start of each audio file. This will make the manipulation of annotations much easier and cleaner.\n",
    "\n",
    "1. Open all files for an entire day in Raven\n",
    "2. Load the annotation table \"...masterlog.txt\" corresponding to that day\n",
    "3. Click right on the header section of the annotation table in Raven, then select \"Choose Measurements...\"\n",
    "4. Add the measurements \"File Offset (s)\", \"Begin File\", and \"Begin Path\".\n",
    "5. Save the updated annotation table (File > Save Selection Table). The modified  Raven table are all saved in .\\USA-NEFSC-GA-201612-CH6\\old_format\\tables_with_added_fields\\."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries and define functions used throughout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from ecosound.core.annotation import Annotation\n",
    "from ecosound.core.metadata import DeploymentInfo\n",
    "from ecosound.core.tools import list_files, filename_to_datetime\n",
    "from ecosound.core.audiotools import Sound\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import re\n",
    "import uuid\n",
    "\n",
    "def get_datetime_from_filename(filename):\n",
    "    time_format = \"%Y%m%d_%H%M%S\"\n",
    "    file_offset_s_regex = \"_[0-9]+s\"\n",
    "    file_offset_ms_regex = \"_[0-9]+s\"\n",
    "    file_orig_regex = \"_[0-9]{8}_[0-9]{6}\"    \n",
    "    # first part - date/time of origninal audio file\n",
    "    p1 = re.compile(file_orig_regex)\n",
    "    datestr_1 = p1.search(filename)\n",
    "    date = datetime.strptime(datestr_1[0][1:],time_format)    \n",
    "    ## second part - nb of seconds\n",
    "    #p1 = re.compile(file_orig_regex)\n",
    "    #datestr_1 = p1.search(df['Begin File'].iloc[0])\n",
    "    #date = datetime.strptime(datestr_1[0][1:-1],time_format)    \n",
    "    return date   \n",
    "\n",
    "def load_raven_table(root_dir,audio_dir,annotation_file,deployment_file):\n",
    "    ## load Raven annotations\n",
    "    df = pd.read_csv(os.path.join(root_dir, annotation_file), sep='\\t')\n",
    "    df = df[df['View']== 'Spectrogram 1'] # remove all \"waveform\" rows (redundant with the \"Spectrogram\" ones)\n",
    "    df = df.reset_index(drop=True)    \n",
    "    ## find out start date/time for each audio file\n",
    "    files_date=df['Begin File'].apply(get_datetime_from_filename)\n",
    "    # Definition of start and stop time offsets of annoatations (relative to start of each audio file)\n",
    "    duration = df['End Time (s)']-df['Begin Time (s)']\n",
    "    start_offset = df['File Offset (s)']\n",
    "    end_offset = start_offset + duration\n",
    "    ## Populate annotation object\n",
    "    annot = Annotation()\n",
    "    annot.data['audio_file_start_date'] = files_date\n",
    "    annot.data['audio_channel'] = df['Channel']-1\n",
    "    annot.data['audio_file_name'] = df['Begin File'].apply(lambda x: os.path.splitext(os.path.basename(x))[0])\n",
    "    annot.data['audio_file_dir'] = audio_dir\n",
    "    annot.data['audio_file_extension'] = df['Begin Path'].apply(lambda x: os.path.splitext(x)[1])\n",
    "    annot.data['time_min_offset'] = start_offset\n",
    "    annot.data['time_max_offset'] = end_offset\n",
    "    annot.data['time_min_date'] = pd.to_datetime(annot.data['audio_file_start_date'] + pd.to_timedelta(annot.data['time_min_offset'], unit='s'))\n",
    "    annot.data['time_max_date'] = pd.to_datetime(annot.data['audio_file_start_date'] + pd.to_timedelta(annot.data['time_max_offset'], unit='s'))\n",
    "    annot.data['frequency_min'] = df['Low Freq (Hz)']\n",
    "    annot.data['frequency_max'] = df['High Freq (Hz)']    \n",
    "    annot.data['label_class'] = 'MW'\n",
    "    annot.data['label_subclass'] = df['calltype']\n",
    "    annot.data['from_detector'] = False\n",
    "    annot.data['software_name'] = 'raven'\n",
    "    annot.data['uuid'] = annot.data.apply(lambda _: str(uuid.uuid4()), axis=1)\n",
    "    annot.data['duration'] = annot.data['time_max_offset'] - annot.data['time_min_offset']\n",
    "    annot.insert_metadata(os.path.join(root_dir, deployment_file)) # insert metadata\n",
    "    annot.check_integrity(verbose=True, ignore_frequency_duplicates=True) # check integrity\n",
    "    print(len(annot), 'annotations imported.')\n",
    "    return annot\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create deployment info files with metadata for each deployment\n",
    "\n",
    "Instantiate a DeploymentInfo object to handle metadata for the deployment, and create an empty deployment info file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate\n",
    "Deployment = DeploymentInfo()\n",
    "# write empty file to fill in (do once only)\n",
    "#Deployment.write_template(os.path.join(root_dir, deployment_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A csv file \"deployment_info.csv\" has now been created in the root_dir. It is empty and only has column headers, and includes teh following fiilds:\n",
    "\n",
    "* audio_channel_number\n",
    "* UTC_offset\n",
    "* sampling_frequency (in Hz)\n",
    "* bit_depth \n",
    "* mooring_platform_name\n",
    "* recorder_type\n",
    "* recorder_SN\n",
    "* hydrophone_model\n",
    "* hydrophone_SN\n",
    "* hydrophone_depth\n",
    "* location_name\n",
    "* location_lat\n",
    "* location_lon\n",
    "* location_water_depth\n",
    "* deployment_ID\n",
    "* deployment_date\n",
    "* recovery_date\n",
    "\n",
    "This file needs to be filled in by the user with the appropriate deployment information. Once filled in, the file can be loaded using the Deployment object:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning annotations\n",
    "\n",
    "Now we go through the modified annotations and add the associated metadata, correct inconsistencies in annotations labels, and save as a NetCDF and Raven file.\n",
    "\n",
    "Definition of all the paths of all folders with the raw annotation and audio files for this deployment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = r'C:\\Users\\xavier.mouy\\Documents\\GitHub\\minke-whale-dataset\\datasets\\USA-NEFSC-GA-201612-CH6'\n",
    "annotation_dir = r'C:\\Users\\xavier.mouy\\Documents\\GitHub\\minke-whale-dataset\\datasets\\USA-NEFSC-GA-201612-CH6\\old_format\\tables_with_added_fields'\n",
    "audio_dir = r'Z:\\ACOUSTIC_DATA\\BOTTOM_MOUNTED\\NEFSC_GA\\NEFSC_GA_201611\\Brunswick'\n",
    "deployment_file = r'deployment_info.csv' \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can load and format the manual annotations for this dataset and add the metadata. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate entries removed: 0\n",
      "Integrity test succesfull\n",
      "274 annotations imported.\n",
      "Duplicate entries removed: 0\n",
      "Integrity test succesfull\n",
      "257 annotations imported.\n",
      "Duplicate entries removed: 0\n",
      "Integrity test succesfull\n",
      "219 annotations imported.\n",
      "Duplicate entries removed: 0\n",
      "Integrity test succesfull\n",
      "198 annotations imported.\n",
      "Duplicate entries removed: 0\n",
      "Integrity test succesfull\n",
      "417 annotations imported.\n",
      "Duplicate entries removed: 0\n",
      "Integrity test succesfull\n",
      "309 annotations imported.\n",
      "Duplicate entries removed: 0\n",
      "Integrity test succesfull\n",
      "326 annotations imported.\n",
      "Duplicate entries removed: 0\n",
      "Integrity test succesfull\n",
      "309 annotations imported.\n",
      "Duplicate entries removed: 0\n",
      "Integrity test succesfull\n",
      "393 annotations imported.\n",
      "Duplicate entries removed: 0\n",
      "Integrity test succesfull\n",
      "293 annotations imported.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Annotation object (2995)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annot = Annotation()\n",
    "annot_files = list_files(annotation_dir,'.txt',recursive=False,case_sensitive=True)\n",
    "for annot_file in annot_files:\n",
    "    annot_tmp = load_raven_table(root_dir,audio_dir,annot_file,deployment_file)\n",
    "    annot = annot + annot_tmp\n",
    "annot.check_integrity()\n",
    "annot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the different annotation labels that were used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MW']\n"
     ]
    }
   ],
   "source": [
    "print(annot.get_labels_class())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, having a look a summary of all the annotations available in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_class                MW  Total\n",
      "deployment_ID                       \n",
      "USA-NEFSC-GA-201611-CH6  2995   2995\n",
      "Total                    2995   2995\n"
     ]
    }
   ],
   "source": [
    "# print summary (pivot table)\n",
    "print(annot.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset can now be saved as a Raven annotation file and netcdf4 file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "annot.to_netcdf(os.path.join(root_dir, 'Annotations_dataset_' + annot.data['deployment_ID'][0] +' annotations.nc'))\n",
    "annot.to_raven(root_dir, outfile='Annotations_dataset_' + annot.data['deployment_ID'][0] +'.Table.1.selections.txt', single_file=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
