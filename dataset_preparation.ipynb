{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation of the annotation dataset\n",
    "This notebook describes the steps involved in gathering, cleaning, and merging all manual annotations made by various analysist from various groups and using different software in a single large dataset.\n",
    "\n",
    "objective of this notebook:\n",
    "\n",
    "- make annotation labels consistent (standard name)\n",
    "- add metadata for proper repartition of train/test/evaluation datasets\n",
    "\n",
    "\n",
    "\n",
    "## Description of the data and manual annotations\n",
    "\n",
    "-- TO DO -- \n",
    "\n",
    "Describe dataset and annotations\n",
    "\n",
    "- ref to Popescu's paper\n",
    "- screenshot of folder\n",
    "\n",
    "## Adding annotation fields in Raven\n",
    "\n",
    "-- TO DO --\n",
    "\n",
    "- screenshot of Raven with entire dataset loaded\n",
    "- screeshot of added fields: file offset, Begin file, Begin path \n",
    "\n",
    "First, ecosound and the other linbraries need to be imported.\n",
    "\n",
    "## Import libraries and define functions used throughout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from ecosound.core.annotation import Annotation\n",
    "from ecosound.core.metadata import DeploymentInfo\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import re\n",
    "import uuid\n",
    "\n",
    "def get_datetime_from_filename(filename):\n",
    "    time_format = \"%Y%m%d_%H%M%S\"\n",
    "    file_offset_s_regex = \"_[0-9]+s\"\n",
    "    file_offset_ms_regex = \"_[0-9]+s\"\n",
    "    file_orig_regex = \"_[0-9]{8}_[0-9]{6}_\"    \n",
    "    # first part - date/time of origninal audio file\n",
    "    p1 = re.compile(file_orig_regex)\n",
    "    datestr_1 = p1.search(filename)\n",
    "    date = datetime.strptime(datestr_1[0][1:-1],time_format)    \n",
    "    ## second part - nb of seconds\n",
    "    #p1 = re.compile(file_orig_regex)\n",
    "    #datestr_1 = p1.search(df['Begin File'].iloc[0])\n",
    "    #date = datetime.strptime(datestr_1[0][1:-1],time_format)    \n",
    "    return date   \n",
    "\n",
    "def load_raven_table(root_dir,audio_dir,annotation_file,deployment_file):\n",
    "    ## load Raven annotations\n",
    "    df = pd.read_csv(os.path.join(root_dir, annotation_file), sep='\\t')\n",
    "    df = df[df['View']== 'Spectrogram 1'] # remove all \"waveform\" rows (redundant with the \"Spectrogram\" ones)\n",
    "    df = df.reset_index(drop=True)    \n",
    "    ## find out start date/time for each audio file\n",
    "    files_date=df['Begin File'].apply(get_datetime_from_filename)\n",
    "    # Definition of start and stop time offsets of annoatations (relative to start of each audio file)\n",
    "    duration = df['End Time (s)']-df['Begin Time (s)']\n",
    "    start_offset = df['File Offset (s)']\n",
    "    end_offset = start_offset + duration\n",
    "    ## Populate annotation object\n",
    "    annot = Annotation()\n",
    "    annot.data['audio_file_start_date'] = files_date\n",
    "    annot.data['audio_channel'] = df['Channel']-1\n",
    "    annot.data['audio_file_name'] = df['Begin File'].apply(lambda x: os.path.splitext(os.path.basename(x))[0])\n",
    "    annot.data['audio_file_dir'] = audio_dir\n",
    "    annot.data['audio_file_extension'] = df['Begin Path'].apply(lambda x: os.path.splitext(x)[1])\n",
    "    annot.data['time_min_offset'] = start_offset\n",
    "    annot.data['time_max_offset'] = end_offset\n",
    "    annot.data['time_min_date'] = pd.to_datetime(annot.data['audio_file_start_date'] + pd.to_timedelta(annot.data['time_min_offset'], unit='s'))\n",
    "    annot.data['time_max_date'] = pd.to_datetime(annot.data['audio_file_start_date'] + pd.to_timedelta(annot.data['time_max_offset'], unit='s'))\n",
    "    annot.data['frequency_min'] = df['Low Freq (Hz)']\n",
    "    annot.data['frequency_max'] = df['High Freq (Hz)']    \n",
    "    annot.data['label_class'] = df['tags']\n",
    "    annot.data['from_detector'] = False\n",
    "    annot.data['software_name'] = 'raven'\n",
    "    annot.data['uuid'] = annot.data.apply(lambda _: str(uuid.uuid4()), axis=1)\n",
    "    annot.data['duration'] = annot.data['time_max_offset'] - annot.data['time_min_offset']\n",
    "    annot.insert_metadata(os.path.join(root_dir, deployment_file)) # insert metadata\n",
    "    annot.check_integrity(verbose=True, ignore_frequency_duplicates=True) # check integrity\n",
    "    print(len(annot), 'annotations imported.')\n",
    "    return annot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create deployment info files with metadata for each deployment\n",
    "\n",
    "Instantiate a DeploymentInfo object to handle metadata for the deployment, and create an empty deployment info file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate\n",
    "Deployment = DeploymentInfo()\n",
    "# write empty file to fill in (do once only)\n",
    "#Deployment.write_template(os.path.join(root_dir, deployment_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A csv file \"deployment_info.csv\" has now been created in the root_dir. It is empty and only has column headers, and includes teh following fiilds:\n",
    "\n",
    "* audio_channel_number\n",
    "* UTC_offset\n",
    "* sampling_frequency (in Hz)\n",
    "* bit_depth \n",
    "* mooring_platform_name\n",
    "* recorder_type\n",
    "* recorder_SN\n",
    "* hydrophone_model\n",
    "* hydrophone_SN\n",
    "* hydrophone_depth\n",
    "* location_name\n",
    "* location_lat\n",
    "* location_lon\n",
    "* location_water_depth\n",
    "* deployment_ID\n",
    "* deployment_date\n",
    "* recovery_date\n",
    "\n",
    "This file needs to be filled in by the user with the appropriate deployment information. Once filled in, the file can be loaded using the Deployment object:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning\n",
    "\n",
    "Annotations are for each datasets are loaded, sorted, and re-writen into a parquet file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 1: USA-SBNMS-NOPP4-20080917\n",
    "\n",
    "Definition of all the paths of all folders with the raw annotation and audio files for this deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = r'C:\\Users\\xavier.mouy\\Documents\\GitHub\\minke-whale-dataset\\datasets\\USA-SBNMS-NOPP4-20080917'\n",
    "audio_dir = r'C:\\Users\\xavier.mouy\\Documents\\GitHub\\minke-whale-dataset\\datasets\\USA-SBNMS-NOPP4-20080917'\n",
    "deployment_file = r'deployment_info.csv' \n",
    "annotation_file = r'SET01_masterlog_new.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can load and format the manual annotations for this dataset and add the metadata. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate entries removed: 0\n",
      "Integrity test succesfull\n",
      "1420 annotations imported.\n"
     ]
    }
   ],
   "source": [
    "annot = load_raven_table(root_dir,audio_dir,annotation_file,deployment_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the different annotation labels that were used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bac_3100', nan]\n"
     ]
    }
   ],
   "source": [
    "print(annot.get_labels_class())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's change 'Bac_3100' to 'MW' (Minke whale) and nan for 'NN' (noise):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "annot.data['label_class'].replace(to_replace=['Bac_3100'], value='MW', inplace=True)\n",
    "annot.data.loc[annot.data['label_class'].isnull(),'label_class'] = 'NN'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, having a look a summary of all the annotations available in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_class                 MW   NN  Total\n",
      "deployment_ID                             \n",
      "USA-SBNMS-NOPP4-20080917  1139  281   1420\n",
      "Total                     1139  281   1420\n"
     ]
    }
   ],
   "source": [
    "# print summary (pivot table)\n",
    "print(annot.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset can now be saved as a Raven annotation file and netcdf4 file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xavier.mouy\\Anaconda3\\envs\\ecosound-p38\\lib\\site-packages\\pandas\\core\\arrays\\timedeltas.py:700: RuntimeWarning: invalid value encountered in multiply\n",
      "  return self - (self // other) * other\n"
     ]
    }
   ],
   "source": [
    "annot.to_netcdf(os.path.join(root_dir, 'Annotations_dataset_' + annot.data['deployment_ID'][0] +' annotations.nc'))\n",
    "annot.to_raven(root_dir, outfile='Annotations_dataset_' + annot.data['deployment_ID'][0] +'.Table.1.selections.txt', single_file=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 2: USA-SBNMS-NOPP5-20090215\n",
    "Now we can repeat the step above for all the other datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = r'C:\\Users\\xavier.mouy\\Documents\\GitHub\\minke-whale-dataset\\datasets\\USA-SBNMS-NOPP5-20090215'\n",
    "audio_dir = r'C:\\Users\\xavier.mouy\\Documents\\GitHub\\minke-whale-dataset\\datasets\\USA-SBNMS-NOPP5-20090215'\n",
    "deployment_file = r'deployment_info.csv' \n",
    "annotation_file = r'SET02_masterlog_new.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate entries removed: 0\n",
      "Integrity test succesfull\n",
      "591 annotations imported.\n"
     ]
    }
   ],
   "source": [
    "annot = load_raven_table(root_dir,audio_dir,annotation_file,deployment_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['minke_pt', 'Bac_3100', nan]\n"
     ]
    }
   ],
   "source": [
    "print(annot.get_labels_class())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "annot.data['label_class'].replace(to_replace=['Bac_3100', 'minke_pt'], value='MW', inplace=True)\n",
    "annot.data.loc[annot.data['label_class'].isnull(),'label_class'] = 'NN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_class                MW   NN  Total\n",
      "deployment_ID                            \n",
      "USA-SBNMS-NOPP5-20090215  432  159    591\n",
      "Total                     432  159    591\n"
     ]
    }
   ],
   "source": [
    "# print summary (pivot table)\n",
    "print(annot.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xavier.mouy\\Anaconda3\\envs\\ecosound-p38\\lib\\site-packages\\pandas\\core\\arrays\\timedeltas.py:700: RuntimeWarning: invalid value encountered in multiply\n",
      "  return self - (self // other) * other\n"
     ]
    }
   ],
   "source": [
    "annot.to_netcdf(os.path.join(root_dir, 'Annotations_dataset_' + annot.data['deployment_ID'][0] +' annotations.nc'))\n",
    "annot.to_raven(root_dir, outfile='Annotations_dataset_' + annot.data['deployment_ID'][0] +'.Table.1.selections.txt', single_file=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging all datasets together\n",
    "\n",
    "Now that all our datasets are cleaned up, we can merge them all in a single Master annotation dataset.\n",
    "\n",
    "Defining the path of each dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = r'C:\\Users\\xavier.mouy\\Documents\\PhD\\Projects\\Dectector\\datasets'\n",
    "dataset_files = ['UVIC_mill-bay_2019\\Annotations_dataset_06-MILL.nc',\n",
    "                 'UVIC_hornby-island_2019\\Annotations_dataset_07-HI.nc',\n",
    "                 'ONC_delta-node_2014\\Annotations_dataset_ONC-Delta-2014.nc',\n",
    "                 'DFO_snake-island_rca-in_20181017\\Annotations_dataset_SI-RCAIn-20181017.nc',\n",
    "                 'DFO_snake-island_rca-out_20181015\\Annotations_dataset_SI-RCAOut-20181015.nc',\n",
    "                ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looping through each dataset and merging in to a master dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load all annotations\n",
    "annot = Annotation()\n",
    "for file in dataset_files:\n",
    "    tmp = Annotation()\n",
    "    tmp.from_netcdf(os.path.join(root_dir, file), verbose=True)\n",
    "    annot = annot + tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see a summary of all the annotatiosn we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print summary (pivot table)\n",
    "print(annot.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at the contribution from each analyst:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(annot.summary(rows='operator_name'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can save our Master annotation dataset. It will be used for trainning and evealuation classification models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#annot.to_parquet(os.path.join(root_dir, 'Master_annotations_dataset.parquet'))\n",
    "annot.to_netcdf(os.path.join(root_dir, 'Master_annotations_dataset.nc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
