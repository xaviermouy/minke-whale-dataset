{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reformating of the SAMS N1 minke annotations\n",
    "\n",
    "\n",
    "## Purpose of this notebook\n",
    "This notebook describes the steps involved in gathering, cleaning up and reorganizing the minke whale pulse train manual annotations that were performed off Scotland by SAMS and the University of Aberdeen. Data were fully analysed from three ~2-week periods. This fully annotated dataset can be used to assess the performance of the detector (i.e. quantify precision, recall, and number of false alarms per day)\n",
    "\n",
    "The specific objectives of this notebook are:\n",
    "\n",
    "- Convert the Raven annotation tables to have annotation times relative to the beginning of each audio file.\n",
    "- Add metadata to all annotations (i.e., coordinates, depths, location, dates, etc)\n",
    "- Make annotation labels consistent (i.e. 'MW' for minke)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries and define functions used throughout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from ecosound.core.annotation import Annotation\n",
    "from ecosound.core.metadata import DeploymentInfo\n",
    "from ecosound.core.tools import list_files, filename_to_datetime\n",
    "from ecosound.core.audiotools import Sound\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import re\n",
    "import uuid\n",
    "\n",
    "def get_datetime_from_filename(filename):\n",
    "    time_format = \"%Y-%m-%d_%H-%M-%S\"    \n",
    "    file_orig_regex = \"_[0-9]{4}-[0-9]{2}-[0-9]{2}_[0-9]{2}-[0-9]{2}-[0-9]{2}\"    \n",
    "    p1 = re.compile(file_orig_regex)\n",
    "    datestr_1 = p1.search(filename)\n",
    "    date = datetime.strptime(datestr_1[0][1:],time_format)      \n",
    "    return date   \n",
    "\n",
    "def load_raven_table(root_dir,audio_dir,annotation_file,deployment_file):\n",
    "    ## load Raven annotations\n",
    "    df = pd.read_csv(os.path.join(root_dir, annotation_file), sep='\\t')\n",
    "    df = df[df['View']== 'Spectrogram 1'] # remove all \"waveform\" rows (redundant with the \"Spectrogram\" ones)\n",
    "    df = df.reset_index(drop=True)    \n",
    "    ## find out start date/time for each audio file    \n",
    "    #print(df['Begin File'])\n",
    "    files_date=df['Begin File'].apply(get_datetime_from_filename)\n",
    "    # Definition of start and stop time offsets of annoatations (relative to start of each audio file)\n",
    "    duration = df['End Time (s)']-df['Begin Time (s)']\n",
    "    start_offset = df['File Offset (s)']\n",
    "    end_offset = start_offset + duration\n",
    "    ## Populate annotation object\n",
    "    annot = Annotation()\n",
    "    annot.data['audio_file_start_date'] = files_date\n",
    "    annot.data['audio_channel'] = df['Channel']-1\n",
    "    annot.data['audio_file_name'] = df['Begin File'].apply(lambda x: os.path.splitext(os.path.basename(x))[0])\n",
    "    annot.data['audio_file_dir'] = audio_dir\n",
    "    annot.data['audio_file_extension'] = df['Begin Path'].apply(lambda x: os.path.splitext(x)[1])\n",
    "    annot.data['time_min_offset'] = start_offset\n",
    "    annot.data['time_max_offset'] = end_offset\n",
    "    annot.data['time_min_date'] = pd.to_datetime(annot.data['audio_file_start_date'] + pd.to_timedelta(annot.data['time_min_offset'], unit='s'))\n",
    "    annot.data['time_max_date'] = pd.to_datetime(annot.data['audio_file_start_date'] + pd.to_timedelta(annot.data['time_max_offset'], unit='s'))\n",
    "    annot.data['frequency_min'] = df['Low Freq (Hz)']\n",
    "    annot.data['frequency_max'] = df['High Freq (Hz)']    \n",
    "    annot.data['label_class'] = 'MW'\n",
    "    #annot.data['label_subclass'] = df['calltype']\n",
    "    annot.data['label_subclass'] = ''\n",
    "    annot.data['from_detector'] = False\n",
    "    annot.data['software_name'] = 'raven'\n",
    "    annot.data['uuid'] = annot.data.apply(lambda _: str(uuid.uuid4()), axis=1)\n",
    "    annot.data['duration'] = annot.data['time_max_offset'] - annot.data['time_min_offset']\n",
    "    annot.data['confidence'] = df['Confidence_rating']\n",
    "    #print(df.columns)\n",
    "    annot.insert_metadata(os.path.join(root_dir, deployment_file)) # insert metadata\n",
    "    annot.check_integrity(verbose=True, ignore_frequency_duplicates=True) # check integrity\n",
    "    print(len(annot), 'annotations imported.')\n",
    "    return annot\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create deployment info files with metadata for each deployment\n",
    "\n",
    "Instantiate a DeploymentInfo object to handle metadata for the deployment, and create an empty deployment info file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate\n",
    "Deployment = DeploymentInfo()\n",
    "# write empty file to fill in (do once only)\n",
    "#Deployment.write_template(os.path.join(root_dir, deployment_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A csv file \"deployment_info.csv\" has now been created in the root_dir. It is empty and only has column headers, and includes teh following fiilds:\n",
    "\n",
    "* audio_channel_number\n",
    "* UTC_offset\n",
    "* sampling_frequency (in Hz)\n",
    "* bit_depth \n",
    "* mooring_platform_name\n",
    "* recorder_type\n",
    "* recorder_SN\n",
    "* hydrophone_model\n",
    "* hydrophone_SN\n",
    "* hydrophone_depth\n",
    "* location_name\n",
    "* location_lat\n",
    "* location_lon\n",
    "* location_water_depth\n",
    "* deployment_ID\n",
    "* deployment_date\n",
    "* recovery_date\n",
    "\n",
    "This file needs to be filled in by the user with the appropriate deployment information. Once filled in, the file can be loaded using the Deployment object:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Annotated period 1 (UK-SAMS-N1-20201102)\n",
    "\n",
    "Definition of all the paths of all folders with the raw annotation and audio files for this deployment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = r'D:\\NOAA\\2022_Minke_whale_detector\\manual_annotations\\continuous_datasets\\UK-SAMS-N1-20201102'\n",
    "annotation_dir = r'D:\\NOAA\\2022_Minke_whale_detector\\manual_annotations\\continuous_datasets\\UK-SAMS-N1-20201102\\old_format'\n",
    "audio_dir = r'D:\\NOAA\\2022_Minke_whale_detector\\manual_annotations\\continuous_datasets\\UK-SAMS-N1-20201102'\n",
    "deployment_file = r'deployment_info.csv' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can load and format the manual annotations for this dataset and add the metadata. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate entries removed: 0\n",
      "Integrity test succesfull\n",
      "3 annotations imported.\n",
      "Duplicate entries removed: 0\n",
      "Integrity test succesfull\n",
      "15 annotations imported.\n",
      "Duplicate entries removed: 0\n",
      "Integrity test succesfull\n",
      "3 annotations imported.\n",
      "Duplicate entries removed: 0\n",
      "Integrity test succesfull\n",
      "79 annotations imported.\n",
      "Duplicate entries removed: 0\n",
      "Integrity test succesfull\n",
      "40 annotations imported.\n",
      "Duplicate entries removed: 0\n",
      "Integrity test succesfull\n",
      "278 annotations imported.\n",
      "Duplicate entries removed: 0\n",
      "Integrity test succesfull\n",
      "157 annotations imported.\n",
      "Duplicate entries removed: 0\n",
      "Integrity test succesfull\n",
      "127 annotations imported.\n",
      "Duplicate entries removed: 0\n",
      "Integrity test succesfull\n",
      "55 annotations imported.\n",
      "Duplicate entries removed: 0\n",
      "Integrity test succesfull\n",
      "130 annotations imported.\n",
      "Duplicate entries removed: 0\n",
      "Integrity test succesfull\n",
      "23 annotations imported.\n",
      "Duplicate entries removed: 0\n",
      "Integrity test succesfull\n",
      "101 annotations imported.\n",
      "Duplicate entries removed: 0\n",
      "Integrity test succesfull\n",
      "14 annotations imported.\n",
      "Duplicate entries removed: 0\n",
      "Integrity test succesfull\n",
      "240 annotations imported.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Annotation object (1265)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annot = Annotation()\n",
    "annot_files = list_files(annotation_dir,'.txt',recursive=False,case_sensitive=True)\n",
    "for annot_file in annot_files:\n",
    "    annot_tmp = load_raven_table(root_dir,audio_dir,annot_file,deployment_file)\n",
    "    annot = annot + annot_tmp\n",
    "annot.check_integrity()\n",
    "annot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the different annotation labels that were used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MW']\n"
     ]
    }
   ],
   "source": [
    "print(annot.get_labels_class())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, having a look a summary of all the annotations available in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_class                       MW  Total\n",
      "deployment_ID                              \n",
      "UK-SAMS-WestScotland-202011-N1  1265   1265\n",
      "Total                           1265   1265\n"
     ]
    }
   ],
   "source": [
    "# print summary (pivot table)\n",
    "print(annot.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset can now be saved as a Raven annotation file and netcdf4 file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xavier.mouy\\AppData\\Local\\anaconda3\\envs\\ecosound-dev\\lib\\site-packages\\xarray\\coding\\times.py:618: RuntimeWarning: invalid value encountered in cast\n",
      "  int_num = np.asarray(num, dtype=np.int64)\n"
     ]
    }
   ],
   "source": [
    "annot.to_netcdf(os.path.join(root_dir, 'Annotations_dataset_' + annot.data['deployment_ID'][0] +' annotations.nc'))\n",
    "annot.to_raven(root_dir, outfile='Annotations_dataset_' + annot.data['deployment_ID'][0] +'.Table.1.selections.txt', single_file=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotated period 2 (UK-SAMS-N1-20201228)\n",
    "\n",
    "No minke whale calls found in this dataset. Nothing to do here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Annotated period 3 (UK-SAMS-N1-20210522)\n",
    "\n",
    "Definition of all the paths of all folders with the raw annotation and audio files for this deployment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = r'D:\\NOAA\\2022_Minke_whale_detector\\manual_annotations\\continuous_datasets\\UK-SAMS-N1-20210522'\n",
    "annotation_dir = r'D:\\NOAA\\2022_Minke_whale_detector\\manual_annotations\\continuous_datasets\\UK-SAMS-N1-20210522\\old_format'\n",
    "audio_dir = r'D:\\NOAA\\2022_Minke_whale_detector\\manual_annotations\\continuous_datasets\\UK-SAMS-N1-20210522'\n",
    "deployment_file = r'deployment_info.csv' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can load and format the manual annotations for this dataset and add the metadata. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate entries removed: 0\n",
      "Integrity test succesfull\n",
      "3 annotations imported.\n",
      "Duplicate entries removed: 0\n",
      "Integrity test succesfull\n",
      "1 annotations imported.\n",
      "Duplicate entries removed: 0\n",
      "Integrity test succesfull\n",
      "1 annotations imported.\n",
      "Duplicate entries removed: 0\n",
      "Integrity test succesfull\n",
      "1 annotations imported.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Annotation object (6)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annot = Annotation()\n",
    "annot_files = list_files(annotation_dir,'.txt',recursive=False,case_sensitive=True)\n",
    "for annot_file in annot_files:\n",
    "    annot_tmp = load_raven_table(root_dir,audio_dir,annot_file,deployment_file)\n",
    "    annot = annot + annot_tmp\n",
    "annot.check_integrity()\n",
    "annot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the different annotation labels that were used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MW']\n"
     ]
    }
   ],
   "source": [
    "print(annot.get_labels_class())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, having a look a summary of all the annotations available in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_class                     MW  Total\n",
      "deployment_ID                            \n",
      "UK-SAMS-WestScotland-202105-N1   6      6\n",
      "Total                            6      6\n"
     ]
    }
   ],
   "source": [
    "# print summary (pivot table)\n",
    "print(annot.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset can now be saved as a Raven annotation file and netcdf4 file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xavier.mouy\\AppData\\Local\\anaconda3\\envs\\ecosound-dev\\lib\\site-packages\\xarray\\coding\\times.py:618: RuntimeWarning: invalid value encountered in cast\n",
      "  int_num = np.asarray(num, dtype=np.int64)\n"
     ]
    }
   ],
   "source": [
    "annot.to_netcdf(os.path.join(root_dir, 'Annotations_dataset_' + annot.data['deployment_ID'][0] +' annotations.nc'))\n",
    "annot.to_raven(root_dir, outfile='Annotations_dataset_' + annot.data['deployment_ID'][0] +'.Table.1.selections.txt', single_file=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
